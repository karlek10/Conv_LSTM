{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fe94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c068abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3f5e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b5fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, seq_len_out, device, dropout=0.1):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Initializing the model parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len_out = seq_len_out\n",
    "        self.device = device\n",
    "        # Layer 1: LSTM # batch_size first ()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, dropout=dropout)\n",
    "        # # Layer 2: Fully coneccted (linear) layer\n",
    "        self.linear = nn.Linear(self.hidden_size, self.seq_len_out)\n",
    "    def forward(self, input_seq, prints = True):\n",
    "        # Reshaping the input_seq\n",
    "        input_seq = input_seq.view(-1, input_seq.shape[1], self.input_size)\n",
    "        if prints: print(\"input_seq shape:\", input_seq.shape, \"->[num_batches, seq_len, num_features]\")     \n",
    "        # LSTM \n",
    "        output, (h_state, c_state) = self.lstm(input_seq)\n",
    "        if prints: print(\"LSTM: output shape:\" , output.shape, \"->[num_batches, seq_len, hidden_size]\", \n",
    "                         \"\\n \" \"LSTM: h_state shape:\", h_state.shape, \n",
    "                          \"->[num_layers*num_directions, num_batches, hidden_size]\", \"\\n\"\n",
    "                          \"LSTM: c_state shape:\", c_state.shape, \n",
    "                          \"->[num_layers*num_directions, num_batches, hidden_size]\")\n",
    "        # Reshaping to take last tensor as output\n",
    "        output = output[:, -1, :]\n",
    "        if prints: print(\"LSTM Output reshaped:\", output.shape, \"->[num_batches, hidden_size]\")\n",
    "        # Fully connected layer\n",
    "        output = self.linear(output)\n",
    "        if prints: print(\"FNN: Final outpu shape:\", output.shape, \"->[num_batches, num_features]\")\n",
    "        print (\"type of the LSTM output is: \", type(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1d3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, dropout = None):\n",
    "        \n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.padding = (kernel_size[0] // 2, kernel_size[1]  // 2) # mantaining same padding\n",
    "        \n",
    "        \n",
    "        # stride = 1, bias = True\n",
    "        self.conv = nn.Conv2d(in_channels = self.input_dim + self.hidden_dim, \n",
    "                              out_channels = self.hidden_dim * 4, \n",
    "                              kernel_size = self.kernel_size,\n",
    "                              padding = self.padding,\n",
    "                              bias = True)\n",
    "        \n",
    "        if (dropout == None):\n",
    "            self.dropout = nn.Dropout(0)\n",
    "        else:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "                input_tensor (torch.Tensor): input,  shape (batch_size, input_channels, height, width)\n",
    "                cur_state  (tuple of torch.Tensors): hidden/cell state (batch_size, hidden_channels, height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "     \n",
    "        combined = self.dropout(torch.cat([input_tensor, h_cur], dim = 1)) \n",
    "        # concatenate x(t) and h(t-1)\n",
    "        # applying dropout before convolution\n",
    "                \n",
    "        combined_conv = self.conv(combined) # convolutional operation on input and prev_hidden\n",
    "        \n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim = 1)\n",
    "        \n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "    \n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "        \n",
    "    def linear(self, batch_size, num_chls, image_size):\n",
    "        height, width = image_size\n",
    "        input_size = height*num_chls*width\n",
    "        return nn.Linear(input_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "239c2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, conv_seq_len_out, \n",
    "                batch_first=True, dropout = None):\n",
    "        super(ConvLSTMLayer, self).__init__()\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "        \n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        print (kernel_size)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        \n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "            \n",
    "        self.input_dim =  input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_seq_len_out = conv_seq_len_out\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "  \n",
    "            cell_list.append(ConvLSTMCell(input_dim = cur_input_dim,\n",
    "                                         hidden_dim = self.hidden_dim[i],\n",
    "                                         kernel_size = self.kernel_size[i], \n",
    "                                         dropout = self.dropout))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state = None):\n",
    "        \n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "        \n",
    "        hidden_state = self._init_hidden(batch_size = b,\n",
    "                                        image_size = (h, w))\n",
    "        \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        \n",
    "        cur_layer_input = input_tensor\n",
    "        \n",
    "        for layer_idx in range(0, self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "\n",
    "            for t in range(0, seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor = cur_layer_input[:, t, :, :, :],\n",
    "                                                cur_state = [h, c])\n",
    "                output_inner.append(h)\n",
    "                print (\"shape of inner output: \", h.shape)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim = 1)\n",
    "            print (\"shape of LAYER output: \", layer_output.shape)\n",
    "            cur_layer_input = layer_output\n",
    "            \n",
    "            \n",
    "            #layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "            \n",
    "        #layer_output_list = layer_output_list[:-1]  \n",
    "        #output_lstm = layer_output_list\n",
    "        last_time_step_output = layer_output[:, -self.conv_seq_len_out:, :, _, _]\n",
    "        print (\"shape ConvLSTM output: \", last_time_step_output.shape, \"[batch_size, seq_len, hidden_size]\")\n",
    "        print (\"ConvLSTM output type: \", type(last_time_step_output))\n",
    "        \n",
    "            \n",
    "        return last_time_step_output\n",
    "            \n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "    \n",
    "    def _linear(self, batch_size, num_chls, image_size):\n",
    "        output = self.cell_list[-1].linear(batch_size, num_chls, image_size)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6f7c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 3), (3, 3), (3, 3)]\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of LAYER output:  torch.Size([14, 5, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of LAYER output:  torch.Size([14, 5, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of inner output:  torch.Size([14, 15, 65, 97])\n",
      "shape of LAYER output:  torch.Size([14, 5, 15, 65, 97])\n",
      "shape ConvLSTM output:  torch.Size([14, 5, 15]) [batch_size, seq_len, hidden_size]\n",
      "ConvLSTM output type:  <class 'torch.Tensor'>\n",
      "input_seq shape: torch.Size([14, 5, 9]) ->[num_batches, seq_len, num_features]\n",
      "LSTM: output shape: torch.Size([14, 5, 15]) ->[num_batches, seq_len, hidden_size] \n",
      " LSTM: h_state shape: torch.Size([3, 14, 15]) ->[num_layers*num_directions, num_batches, hidden_size] \n",
      "LSTM: c_state shape: torch.Size([3, 14, 15]) ->[num_layers*num_directions, num_batches, hidden_size]\n",
      "LSTM Output reshaped: torch.Size([14, 15]) ->[num_batches, hidden_size]\n",
      "FNN: Final outpu shape: torch.Size([14, 3]) ->[num_batches, num_features]\n",
      "type of the LSTM output is:  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 14                # how many timeserieses to be trained in one iteration\n",
    "input_size = 9                  # number of features (stations) --> rain, temp, hum, etc. \n",
    "hidden_size = 15                # number of hidden neurons\n",
    "num_layers = 3                  # number of LSTM layers\n",
    "seq_len_in = 5                # length of the training time series\n",
    "conv_seq_len_out = seq_len_in\n",
    "dropout = 0\n",
    "\n",
    "shape = (65,97)\n",
    "channels = 1\n",
    "kernel = (3,3)\n",
    "\n",
    "torch.manual_seed(14)\n",
    "random.seed(14)\n",
    "np.random.seed(14)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_conv = torch.randn(batch_size,seq_len_in,channels,shape[0], shape[1])\n",
    "   \n",
    "    input_lstm = torch.randn(batch_size,seq_len_in,input_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    convlstm = ConvLSTMLayer(input_dim = channels,\n",
    "                             hidden_dim = hidden_size,\n",
    "                             kernel_size = kernel, \n",
    "                             num_layers = num_layers,\n",
    "                             conv_seq_len_out= conv_seq_len_out,\n",
    "                             dropout = dropout)\n",
    "    \n",
    "    \n",
    "    lstm = LSTM(input_size = input_size,\n",
    "               hidden_size = hidden_size,\n",
    "               num_layers= num_layers,\n",
    "               seq_len_out = seq_len_out, \n",
    "                device = \"cuda\")\n",
    "    \n",
    "    layer_output_list = convlstm(input_conv)\n",
    "    \n",
    "    #print (layer_output_list)\n",
    "    \n",
    "    lstm_output = lstm(input_lstm)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b505134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce40ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
